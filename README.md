# K-Nearest Neighbors (KNN) Classification on Iris Dataset

This project demonstrates how to implement and visualize the K-Nearest Neighbors (KNN) classification algorithm using the famous Iris dataset. The workflow is designed for beginners and intermediate users to understand each step of the machine learning pipeline, from data loading to model evaluation and visualization.

## Project Structure
- `Iris.csv`: The dataset file containing measurements of iris flowers.
- `KNN_Iris_Classification.ipynb`: Jupyter notebook with all code, explanations, and visualizations.
- `images/`: Folder containing saved plots generated by the notebook.

## Step-by-Step Explanation

### 1. Import Required Libraries
We start by importing essential Python libraries:
- **pandas** and **numpy** for data manipulation
- **matplotlib** and **seaborn** for plotting
- **scikit-learn** for machine learning tools

### 2. Load and Explore the Dataset
- The Iris dataset is loaded from `Iris.csv` using pandas.
- We display the first few rows to get a sense of the data.
- We check for missing values and view basic statistics (mean, std, min, max, etc.).
- The distribution of the three iris species is shown.

### 3. Preprocess and Normalize Features
- We select the four main features: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm.
- The target variable is the species of the iris flower.
- Features are normalized using `StandardScaler` to ensure all have the same scale, which is important for KNN.

### 4. Split Data into Training and Test Sets
- The dataset is split into training (80%) and test (20%) sets using `train_test_split`.
- Stratification ensures each species is proportionally represented in both sets.

### 5. Train KNN Classifier with Different K Values
- We train the KNN classifier for different values of K (from 1 to 15).
- For each K, we record the accuracy on both the training and test sets.
- The results are plotted and saved as `images/knn_accuracy_vs_k.png` to help choose the best K.

### 6. Evaluate Model Performance
- The best K (with highest test accuracy) is selected.
- The model is retrained with this K and evaluated on the test set.
- We print the accuracy and a detailed classification report (precision, recall, f1-score for each class).

### 7. Visualize Confusion Matrix
- The confusion matrix shows how well the model predicts each class.
- It is plotted as a heatmap and saved as `images/confusion_matrix.png`.

### 8. Visualize Decision Boundaries (2D Projection)
- We use PCA (Principal Component Analysis) to reduce the feature space to 2D for visualization.
- The KNN classifier is retrained on the 2D data.
- Decision boundaries are plotted, showing how the model separates the classes, and saved as `images/knn_decision_boundaries.png`.

## How to Run
1. Open `KNN_Iris_Classification.ipynb` in Jupyter Notebook or VS Code.
2. Run each cell in order. The images will be saved automatically in the `images` folder.
3. Review the plots and outputs for a complete understanding of the KNN classification process.

## Key Takeaways
- **KNN** is a simple, intuitive classification algorithm that relies on feature similarity.
- **Feature scaling** is crucial for distance-based algorithms like KNN.
- **Model evaluation** using accuracy, confusion matrix, and visualizations helps in understanding performance and limitations.
- **Visualization** of decision boundaries provides insight into how the model makes predictions.

---

Feel free to explore and modify the notebook to experiment with other datasets or algorithms!
